{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] WARNING | pattern 'utils_edit_mb.ipynb' matched no files\n",
      "This application is used to convert notebook files (*.ipynb) to various other\n",
      "formats.\n",
      "\n",
      "WARNING: THE COMMANDLINE INTERFACE MAY CHANGE IN FUTURE RELEASES.\n",
      "\n",
      "Options\n",
      "-------\n",
      "\n",
      "Arguments that take values are actually convenience aliases to full\n",
      "Configurables, whose aliases are listed on the help line. For more information\n",
      "on full configurables, see '--help-all'.\n",
      "\n",
      "--debug\n",
      "    set log level to logging.DEBUG (maximize logging output)\n",
      "--generate-config\n",
      "    generate default config file\n",
      "-y\n",
      "    Answer yes to any questions instead of prompting.\n",
      "--execute\n",
      "    Execute the notebook prior to export.\n",
      "--allow-errors\n",
      "    Continue notebook execution even if one of the cells throws an error and include the error message in the cell output (the default behaviour is to abort conversion). This flag is only relevant if '--execute' was specified, too.\n",
      "--stdin\n",
      "    read a single notebook file from stdin. Write the resulting notebook with default basename 'notebook.*'\n",
      "--stdout\n",
      "    Write notebook output to stdout instead of files.\n",
      "--inplace\n",
      "    Run nbconvert in place, overwriting the existing notebook (only \n",
      "    relevant when converting to notebook format)\n",
      "--clear-output\n",
      "    Clear output of current file and save in place, \n",
      "    overwriting the existing notebook.\n",
      "--no-prompt\n",
      "    Exclude input and output prompts from converted document.\n",
      "--no-input\n",
      "    Exclude input cells and output prompts from converted document. \n",
      "    This mode is ideal for generating code-free reports.\n",
      "--log-level=<Enum> (Application.log_level)\n",
      "    Default: 30\n",
      "    Choices: (0, 10, 20, 30, 40, 50, 'DEBUG', 'INFO', 'WARN', 'ERROR', 'CRITICAL')\n",
      "    Set the log level by value or name.\n",
      "--config=<Unicode> (JupyterApp.config_file)\n",
      "    Default: ''\n",
      "    Full path of a config file.\n",
      "--to=<Unicode> (NbConvertApp.export_format)\n",
      "    Default: 'html'\n",
      "    The export format to be used, either one of the built-in formats\n",
      "    ['asciidoc', 'custom', 'html', 'latex', 'markdown', 'notebook', 'pdf',\n",
      "    'python', 'rst', 'script', 'slides'] or a dotted object name that represents\n",
      "    the import path for an `Exporter` class\n",
      "--template=<Unicode> (TemplateExporter.template_file)\n",
      "    Default: ''\n",
      "    Name of the template file to use\n",
      "--writer=<DottedObjectName> (NbConvertApp.writer_class)\n",
      "    Default: 'FilesWriter'\n",
      "    Writer class used to write the  results of the conversion\n",
      "--post=<DottedOrNone> (NbConvertApp.postprocessor_class)\n",
      "    Default: ''\n",
      "    PostProcessor class used to write the results of the conversion\n",
      "--output=<Unicode> (NbConvertApp.output_base)\n",
      "    Default: ''\n",
      "    overwrite base name use for output files. can only be used when converting\n",
      "    one notebook at a time.\n",
      "--output-dir=<Unicode> (FilesWriter.build_directory)\n",
      "    Default: ''\n",
      "    Directory to write output(s) to. Defaults to output to the directory of each\n",
      "    notebook. To recover previous default behaviour (outputting to the current\n",
      "    working directory) use . as the flag value.\n",
      "--reveal-prefix=<Unicode> (SlidesExporter.reveal_url_prefix)\n",
      "    Default: ''\n",
      "    The URL prefix for reveal.js (version 3.x). This defaults to the reveal CDN,\n",
      "    but can be any url pointing to a copy  of reveal.js.\n",
      "    For speaker notes to work, this must be a relative path to a local  copy of\n",
      "    reveal.js: e.g., \"reveal.js\".\n",
      "    If a relative path is given, it must be a subdirectory of the current\n",
      "    directory (from which the server is run).\n",
      "    See the usage documentation\n",
      "    (https://nbconvert.readthedocs.io/en/latest/usage.html#reveal-js-html-\n",
      "    slideshow) for more details.\n",
      "--nbformat=<Enum> (NotebookExporter.nbformat_version)\n",
      "    Default: 4\n",
      "    Choices: [1, 2, 3, 4]\n",
      "    The nbformat version to write. Use this to downgrade notebooks.\n",
      "\n",
      "To see all available configurables, use `--help-all`\n",
      "\n",
      "Examples\n",
      "--------\n",
      "\n",
      "    The simplest way to use nbconvert is\n",
      "    \n",
      "    > jupyter nbconvert mynotebook.ipynb\n",
      "    \n",
      "    which will convert mynotebook.ipynb to the default format (probably HTML).\n",
      "    \n",
      "    You can specify the export format with `--to`.\n",
      "    Options include ['asciidoc', 'custom', 'html', 'latex', 'markdown', 'notebook', 'pdf', 'python', 'rst', 'script', 'slides'].\n",
      "    \n",
      "    > jupyter nbconvert --to latex mynotebook.ipynb\n",
      "    \n",
      "    Both HTML and LaTeX support multiple output templates. LaTeX includes\n",
      "    'base', 'article' and 'report'.  HTML includes 'basic' and 'full'. You\n",
      "    can specify the flavor of the format used.\n",
      "    \n",
      "    > jupyter nbconvert --to html --template basic mynotebook.ipynb\n",
      "    \n",
      "    You can also pipe the output to stdout, rather than a file\n",
      "    \n",
      "    > jupyter nbconvert mynotebook.ipynb --stdout\n",
      "    \n",
      "    PDF is generated via latex\n",
      "    \n",
      "    > jupyter nbconvert mynotebook.ipynb --to pdf\n",
      "    \n",
      "    You can get (and serve) a Reveal.js-powered slideshow\n",
      "    \n",
      "    > jupyter nbconvert myslides.ipynb --to slides --post serve\n",
      "    \n",
      "    Multiple notebooks can be given at the command line in a couple of \n",
      "    different ways:\n",
      "    \n",
      "    > jupyter nbconvert notebook*.ipynb\n",
      "    > jupyter nbconvert notebook1.ipynb notebook2.ipynb\n",
      "    \n",
      "    or you can specify the notebooks list in a config file, containing::\n",
      "    \n",
      "        c.NbConvertApp.notebooks = [\"my_notebook.ipynb\"]\n",
      "    \n",
      "    > jupyter nbconvert --config mycfg.py\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning:\n",
      "\n",
      "numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "\n",
      "/anaconda3/lib/python3.7/site-packages/lightgbm/__init__.py:48: UserWarning:\n",
      "\n",
      "Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.3) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.interpolate import splrep, splev\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "from fbprophet import Prophet\n",
    "from scipy.interpolate import InterpolatedUnivariateSpline\n",
    "%matplotlib inline\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "import utils\n",
    "import json\n",
    "from LinearAlignment import LinearAlignment\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion, make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelBinarizer, LabelEncoder\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.svm import LinearSVR\n",
    "from numpy import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smape(satellite_predicted_values, satellite_true_values):\n",
    "    # the division, addition and subtraction are pointwise\n",
    "\n",
    "    return np.mean(np.abs((satellite_predicted_values - satellite_true_values)/\n",
    "                (np.abs(satellite_predicted_values) + np.abs(satellite_true_values))))\n",
    "\n",
    "\n",
    "def drop_close(t, x, eps=10**9):\n",
    "    '''\n",
    "    t = time array, x = data array, eps is in nanoseconds\n",
    "    Returns entries in t,x with corresponding consecutive times > eps\n",
    "    \n",
    "    '''\n",
    "    t = np.array(t) #if not already np array, convert\n",
    "    x = np.array(x) #if t or x are pandas Series, will have dimension mismatch\n",
    "    far = np.concatenate([(t[1:] - t[:-1]) > eps, [True]])\n",
    "    return t[far], x[far]\n",
    "\n",
    "\n",
    "def resample(t, x, step=10 * 10**9, t_new=None, return_t=False):\n",
    "    '''\n",
    "    t: time array (or series); \n",
    "    x: data array (or series); \n",
    "    t_new: new time scale from start to end of t with step size step;\n",
    "    step: = 10 seconds by default; \n",
    "    return_t: by default, do not return resampled times\n",
    "    \n",
    "    resample time series or array by 10 (default) sec intervals and \n",
    "    return new time series (if t_new=True) and spline approximation series for data\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    t, x = drop_close(t, x)\n",
    "    if t_new is None:\n",
    "        t_new = np.arange(t[0], t[-1], step)\n",
    "    try:\n",
    "        spl = splrep(t, x)\n",
    "        x_new = splev(t_new, spl)\n",
    "    except:\n",
    "        raise ValueError(f'interpolation error, x length = {len(x)}, \\\n",
    "        t_new length = {len(t_new)}')\n",
    "\n",
    "    return (t_new, x_new) if return_t else x_new\n",
    "\n",
    "\n",
    "def get_peaks(array):\n",
    "    '''\n",
    "    returns index of \"sharp\" peaks, excluding first and last values of array\n",
    "    \n",
    "    index of \"smooth peaks\", e.g. 1 2 3 9 9 3 2 1, is not returned\n",
    "    '''\n",
    "    return np.where((array[1:-1] > array[2:]) & (array[1:-1] > array[:-2]))[0] + 1\n",
    "\n",
    "\n",
    "def get_satellite_data(data, sat_id):\n",
    "    '''\n",
    "    returns all data for particular satellite by id\n",
    "    '''\n",
    "    return data[data['sat_id'] == sat_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_previous_and_shift(df,col_name,ind):\n",
    "    '''\n",
    "    input a data frame (df), column name (col_name), and index (ind)\n",
    "    insert previous value of df[col_name] at index and shift the rest \n",
    "    of df[col_name] from ind by +1;\n",
    "    This is used for remove_time_jumps_fast\n",
    "    '''\n",
    "    shifted_series = df[col_name].shift(1)\n",
    "    df[col_name].iloc[ind] = df[col_name].iloc[ind-1]\n",
    "    df[col_name].iloc[ind+1:] = shifted_series.iloc[ind+1:]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_time_jumps_fast(data, features_list=\n",
    "                           ('x_sim', 'y_sim', 'z_sim', 'Vx_sim', 'Vy_sim', 'Vz_sim'),\n",
    "                           threshold = 0.000001):\n",
    "    #time_threshold 0.00003 sufficient for test and train\n",
    "    #time_threshold 0.00002 will throw errors\n",
    "    '''\n",
    "    removes time jumps in the simulation for a single satellite\n",
    "    for train and test data, sufficient to set time_threshold at default\n",
    "    s_data = satellite data\n",
    "    the features are replaced by the correction\n",
    "    note that threshold here is not the same as in remove_time_jumps\n",
    "    '''\n",
    "    epoch_ind = data.columns.get_loc('epoch')\n",
    "    data['t'] = ((pd.to_datetime(data['epoch']) - pd.to_datetime(data.iloc[0,epoch_ind])) /\n",
    "                               np.timedelta64(1, 'D')).astype(float)\n",
    "    data['dt'] = data['t'].diff(1)\n",
    "\n",
    "    index_for_correction = data[data['dt'] < threshold].index \n",
    "    #print(index_for_correction)\n",
    "    if list(index_for_correction): #if non empty\n",
    "        for feature in features_list:\n",
    "            for i in index_for_correction:\n",
    "                j = data.index.get_loc(i)\n",
    "                data = insert_previous_and_shift(data,feature,j)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stretch_simulated_feats(data, stretch,true_feats = 'position',\n",
    "                           suffix=\"_stretch\"):\n",
    "    '''\n",
    "    stretch time scale for simulation to better match true data,\n",
    "    for a single sattelite;\n",
    "    use predetermined stretch coefficient (depends on the satellite)\n",
    "    \n",
    "    '''\n",
    "    if true_feats == 'position':\n",
    "        true_feats_list = ['x','y','z']\n",
    "    elif true_feats == 'velocity':\n",
    "        true_feats_list = ['Vx','Vy','Vz']\n",
    "    elif true_feats == 'all':\n",
    "        true_feats_list = ['x','y','z'] + ['Vx','Vy','Vz']\n",
    "    else:\n",
    "        true_feats_list = true_feats\n",
    "    \n",
    "    for feature in true_feats_list:\n",
    "        spl = splrep(stretch*data['t'],data[feature+'_sim'])\n",
    "        test_stretch = splev(data['t'], spl) #np array\n",
    "        data[feature+suffix] = test_stretch\n",
    "    \n",
    "    return data\n",
    "\n",
    "def amp_sim_feats(data, amp_stretch,feats = ['Vx_sim','Vy_sim','Vz_sim'],\n",
    "                           suffix=\"_stretch_amp\"):\n",
    "    '''\n",
    "    vary amplitude for simulation to better match true data,\n",
    "    for a single sattelite;\n",
    "    use predetermined amp_stretch coefficient (depends on the satellite)\n",
    "    \n",
    "    '''\n",
    "    if feats == 'position':\n",
    "        feats_list = ['x_sim','y_sim','z_sim']\n",
    "    elif feats == 'velocity':\n",
    "        feats_list = ['Vx_sim','Vy_sim','Vz_sim']\n",
    "    elif feats == 'all':\n",
    "        feats_list = ['x_sim','y_sim','z_sim'] + ['Vx_sim','Vy_sim','Vz_sim']\n",
    "    else:\n",
    "        feats_list = feats\n",
    "        \n",
    "    for feature in feats_list:\n",
    "        data[feature+suffix] = amp_stretch*data[feature]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_X(df,col='sat_id',ratio = 0.8,discard = 0):\n",
    "    '''\n",
    "    train test split for our train data only, no targets, \n",
    "    default, 80% train for each satellite, 20% test\n",
    "    '''\n",
    "    sat_list = df[col].unique()\n",
    "    X_train = pd.DataFrame([])\n",
    "    X_test = pd.DataFrame([])\n",
    "    for sat in sat_list:\n",
    "        sat_df = get_satellite_data(df,sat)\n",
    "        m = int(discard*sat_df.shape[0])\n",
    "        n = int(ratio*sat_df.shape[0])\n",
    "        X_train = X_train.append(sat_df[m:n])\n",
    "        X_test = X_test.append(sat_df[n:])\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_list = ['x','y','z','Vx','Vy','Vz']\n",
    "feature_list = [t+'_sim' for t in target_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv', index_col = 'id')\n",
    "train['time'] = train['epoch']\n",
    "train['epoch'] = pd.to_datetime(train['epoch']).values.astype(float)\n",
    "test = pd.read_csv('data/test.csv', index_col = 'id')\n",
    "test['time'] = test['epoch']\n",
    "test['epoch'] = pd.to_datetime(test['epoch']).values.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: (649912, 15)\n"
     ]
    }
   ],
   "source": [
    "print('train:',train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Track 1 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_t1 = test['sat_id'].unique()\n",
    "train_t1 = train[train['sat_id'].isin(sat_t1)]\n",
    "train_test = pd.concat([train,test],sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load transformed data, from train (January) and test (February) data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['x_sim', 'y_sim', 'z_sim', 'Vx_sim', 'Vy_sim', 'Vz_sim', 'sat_id',\n",
       "       'epoch'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tf = pd.read_csv('train_transformed.csv')\n",
    "train_tf.drop('id',axis=1,inplace=True)\n",
    "train_tf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tf = pd.read_csv('submission_2020-02-03_12-12-21.csv',index_col='id')\n",
    "test_epoch = pd.read_csv('data/test.csv',index_col='id')['epoch']\n",
    "test_sat_id = pd.read_csv('data/test.csv',index_col='id')['sat_id']\n",
    "test_tf = pd.concat([test_sat_id,test_tf,test_epoch],axis=1)\n",
    "test_tf['epoch'] = pd.to_datetime(test_tf['epoch']).values.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_dict = {c:c_n for (c,c_n) in list(zip(target_list,feature_list))}\n",
    "test_tf = test_tf.rename(rename_dict,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = train.iloc[0,train.columns.get_loc('epoch')]\n",
    "day = (train['epoch'].max() - train['epoch'].min())/31"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tf = train_tf[test_tf.columns]\n",
    "df = pd.concat([train_tf,test_tf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['epoch', 'sat_id','x_sim', 'y_sim',\n",
    "       'z_sim', 'Vx_sim', 'Vy_sim', 'Vz_sim', 'time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tf = train_tf[test_tf.columns]\n",
    "df = pd.concat([train_tf,test_tf])\n",
    "df['r'] = df['x_sim']**2 + df['y_sim']**2 + df['z_sim']**2\n",
    "df['Vr'] = df['Vx_sim']**2 + df['Vy_sim']**2 + df['Vz_sim']**2\n",
    "df['r_ratio_Vr'] = df['r']/df['Vr']\n",
    "df['epoch'] = (df['epoch']-start)/day\n",
    "for col in df.columns:\n",
    "    if col not in ['sat_id','time']:\n",
    "        df[col+'_d1'] = df[col].diff(1)\n",
    "    if col not in ['sat_id','epoch','time']:\n",
    "        df[col+'_d2'] = df[col].diff(2)\n",
    "        df[col+'_d3'] = df[col].diff(3)\n",
    "        df[col+'_d4'] = df[col].diff(4)\n",
    "        df[col+'_d5'] = df[col].diff(5)\n",
    "        df[col+'_shift1'] = df[col].shift(1)\n",
    "        for i in range(2,6):\n",
    "            df[col+'_shift{}'.format(i)] = df[col+'_shift{}'.format(i-1)]\n",
    "df = df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b666bf274d0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(599254, 102)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fe = df[:train_t1.shape[0]]\n",
    "test_fe = df[train_t1.shape[0]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(284071, 102)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_fe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_feats = ['sat_id']\n",
    "num_feats = [f for f in train_fe.columns if f not in cat_feats]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train test split: Discard first 60% of data, then 50/50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame([])\n",
    "X_test = pd.DataFrame([])\n",
    "for sat in train_fe['sat_id'].unique():\n",
    "    train,test = train_test_split_X(get_satellite_data(train_fe,sat),ratio = 0.9,discard=0.8)\n",
    "    X_train = X_train.append(train)\n",
    "    X_test = X_test.append(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31510, 102) (31648, 102)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape,X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['epoch', 'sat_id', 'x', 'y', 'z', 'Vx', 'Vy', 'Vz', 'x_sim', 'y_sim',\n",
       "       'z_sim', 'Vx_sim', 'Vy_sim', 'Vz_sim', 'time'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_t1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pd.DataFrame([])\n",
    "y_test = pd.DataFrame([])\n",
    "for sat in train_fe['sat_id'].unique():\n",
    "    train,test = train_test_split_X(get_satellite_data(train_t1,sat),ratio = 0.9, discard=0.8)\n",
    "    y_train = y_train.append(train)\n",
    "    y_test = y_test.append(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train[target_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31510, 6) (31648, 15)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Columns(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, names=None):\n",
    "        self.names = names\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[self.names]\n",
    "    \n",
    "    \n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"features\", FeatureUnion([\n",
    "        ('numeric', make_pipeline(Columns(names=num_feats),StandardScaler())),\n",
    "        ('categorical', make_pipeline(Columns(names=cat_feats),OneHotEncoder(sparse=False)))\n",
    "    ])),\n",
    "    ('svr', LinearSVR())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('features', FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('numeric', Pipeline(memory=None,\n",
       "     steps=[('columns', Columns(names=['x_sim', 'y_sim', 'z_sim', 'Vx_sim', 'Vy_sim', 'Vz_sim', 'epoch', 'r', 'Vr', 'r_ratio_Vr', 'x_sim_d1', 'x_sim_d2', 'x_sim_d3', 'x_sim_d4', 'x_sim_d5', 'x_sim_s...ing=1.0, loss='epsilon_insensitive', max_iter=1000,\n",
       "     random_state=None, tol=0.0001, verbose=0))])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(X_train,y_train['Vx'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smape for SVR, Vx: 0.08232864669615839\n",
      "(31510,) (31648,)\n",
      "smape for ground truth and simluation, Vx: 0.2776903736414705\n"
     ]
    }
   ],
   "source": [
    "print('smape for SVR, Vx:',smape(pred,y_test['Vx']))\n",
    "x,y = train_test_split_X(train_t1[['Vx_sim','sat_id']],ratio=0.9,discard=0.8)\n",
    "print('smape for ground truth and simluation, Vx:',smape(y['Vx_sim'],y_test['Vx']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9min 25s, sys: 2.78 s, total: 9min 27s\n",
      "Wall time: 2min 53s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=-1,\n",
       "           oob_score=False, random_state=50, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "rf = RandomForestRegressor(n_estimators=50, n_jobs=-1, random_state=50)\n",
    "rf.fit(X_train.fillna(0), y_train['Vx'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smape for RF, Vx: 0.07736737939508413\n",
      "smape for ground truth and simluation, Vx: 0.6774338934855161\n"
     ]
    }
   ],
   "source": [
    "pred = rf.predict(X_test)\n",
    "print('smape for RF, Vx:',smape(pred,y_test['Vx']))\n",
    "print('smape for ground truth and simluation, Vx:',smape(X_test['Vx_sim'],y_test['Vx']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_train = [] #training data simulation \n",
    "for sat in sat_t1:\n",
    "    dfs_train.append(get_satellite_data(train_tf,sat))\n",
    "dfs_test = [] #test data simulation\n",
    "for sat in sat_t1:\n",
    "    dfs_test.append(get_satellite_data(test_tf,sat))\n",
    "dfs_target = [] #training data ground truth\n",
    "for sat in sat_t1: \n",
    "    dfs_target.append(get_satellite_data(train[target_list +['sat_id']],sat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:21<00:00, 13.70it/s]\n"
     ]
    }
   ],
   "source": [
    "for df in tqdm(dfs_train):\n",
    "    for col in df.columns:\n",
    "        \n",
    "        df['r'] = df['x_sim']**2 + df['y_sim']**2 + df['z_sim']**2\n",
    "        df['Vr'] = df['Vx_sim']**2 + df['Vy_sim']**2 + df['Vz_sim']**2\n",
    "        df['r_ratio_Vr'] = df['r']/df['Vr']\n",
    "        df['epoch'] = (df['epoch']-start)/day\n",
    "        df[col+'_d1'] = df[col].diff(1)\n",
    "        if col not in ['sat_id','epoch']:\n",
    "            df[col+'_d2'] = df[col].diff(2)\n",
    "            df[col+'_d3'] = df[col].diff(3)\n",
    "            df[col+'_d4'] = df[col].diff(4)\n",
    "            df[col+'_d5'] = df[col].diff(5)\n",
    "            df[col+'_shift1'] = df[col].shift(1)\n",
    "            for i in range(2,6):\n",
    "                df[col+'_shift{}'.format(i)] = df[col+'_shift{}'.format(i-1)]\n",
    "        df['epoch'] = (df['epoch']-start)/day\n",
    "    df = df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:20<00:00, 14.96it/s]\n"
     ]
    }
   ],
   "source": [
    "for df in tqdm(dfs_test):\n",
    "    for col in df.columns:\n",
    "        \n",
    "        df['r'] = df['x_sim']**2 + df['y_sim']**2 + df['z_sim']**2\n",
    "        df['Vr'] = df['Vx_sim']**2 + df['Vy_sim']**2 + df['Vz_sim']**2\n",
    "        df['r_ratio_Vr'] = df['r']/df['Vr']\n",
    "        df['epoch'] = (df['epoch']-start)/day\n",
    "        df[col+'_d1'] = df[col].diff(1)\n",
    "        if col not in ['sat_id','epoch']:\n",
    "            df[col+'_d2'] = df[col].diff(2)\n",
    "            df[col+'_d3'] = df[col].diff(3)\n",
    "            df[col+'_d4'] = df[col].diff(4)\n",
    "            df[col+'_d5'] = df[col].diff(5)\n",
    "            df[col+'_shift1'] = df[col].shift(1)\n",
    "            for i in range(2,6):\n",
    "                df[col+'_shift{}'.format(i)] = df[col+'_shift{}'.format(i-1)]\n",
    "        df['epoch'] = (df['epoch']-start)/day\n",
    "    df = df.fillna(0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IDAOenv",
   "language": "python",
   "name": "idaoenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
