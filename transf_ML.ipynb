{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/lightgbm/__init__.py:48: UserWarning:\n",
      "\n",
      "Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.3) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.interpolate import splrep, splev\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "from fbprophet import Prophet\n",
    "from scipy.interpolate import InterpolatedUnivariateSpline\n",
    "%matplotlib inline\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "import utils\n",
    "import json\n",
    "from LinearAlignment import LinearAlignment\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelBinarizer, LabelEncoder\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from lightgbm import LGBMRegressor\n",
    "from numpy import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smape(satellite_predicted_values, satellite_true_values):\n",
    "    # the division, addition and subtraction are pointwise\n",
    "\n",
    "    return np.mean(np.abs((satellite_predicted_values - satellite_true_values)/\n",
    "                (np.abs(satellite_predicted_values) + np.abs(satellite_true_values))))\n",
    "\n",
    "\n",
    "def drop_close(t, x, eps=10**9):\n",
    "    '''\n",
    "    t = time array, x = data array, eps is in nanoseconds\n",
    "    Returns entries in t,x with corresponding consecutive times > eps\n",
    "    \n",
    "    '''\n",
    "    t = np.array(t) #if not already np array, convert\n",
    "    x = np.array(x) #if t or x are pandas Series, will have dimension mismatch\n",
    "    far = np.concatenate([(t[1:] - t[:-1]) > eps, [True]])\n",
    "    return t[far], x[far]\n",
    "\n",
    "\n",
    "def resample(t, x, step=10 * 10**9, t_new=None, return_t=False):\n",
    "    '''\n",
    "    t: time array (or series); \n",
    "    x: data array (or series); \n",
    "    t_new: new time scale from start to end of t with step size step;\n",
    "    step: = 10 seconds by default; \n",
    "    return_t: by default, do not return resampled times\n",
    "    \n",
    "    resample time series or array by 10 (default) sec intervals and \n",
    "    return new time series (if t_new=True) and spline approximation series for data\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    t, x = drop_close(t, x)\n",
    "    if t_new is None:\n",
    "        t_new = np.arange(t[0], t[-1], step)\n",
    "    try:\n",
    "        spl = splrep(t, x)\n",
    "        x_new = splev(t_new, spl)\n",
    "    except:\n",
    "        raise ValueError(f'interpolation error, x length = {len(x)}, \\\n",
    "        t_new length = {len(t_new)}')\n",
    "\n",
    "    return (t_new, x_new) if return_t else x_new\n",
    "\n",
    "\n",
    "def get_peaks(array):\n",
    "    '''\n",
    "    returns index of \"sharp\" peaks, excluding first and last values of array\n",
    "    \n",
    "    index of \"smooth peaks\", e.g. 1 2 3 9 9 3 2 1, is not returned\n",
    "    '''\n",
    "    return np.where((array[1:-1] > array[2:]) & (array[1:-1] > array[:-2]))[0] + 1\n",
    "\n",
    "\n",
    "def get_satellite_data(data, sat_id):\n",
    "    '''\n",
    "    returns all data for particular satellite by id\n",
    "    '''\n",
    "    return data[data['sat_id'] == sat_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_previous_and_shift(df,col_name,ind):\n",
    "    '''\n",
    "    input a data frame (df), column name (col_name), and index (ind)\n",
    "    insert previous value of df[col_name] at index and shift the rest \n",
    "    of df[col_name] from ind by +1;\n",
    "    This is used for remove_time_jumps_fast\n",
    "    '''\n",
    "    shifted_series = df[col_name].shift(1)\n",
    "    df[col_name].iloc[ind] = df[col_name].iloc[ind-1]\n",
    "    df[col_name].iloc[ind+1:] = shifted_series.iloc[ind+1:]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_time_jumps_fast(data, features_list=\n",
    "                           ('x_sim', 'y_sim', 'z_sim', 'Vx_sim', 'Vy_sim', 'Vz_sim'),\n",
    "                           threshold = 0.000001):\n",
    "    #time_threshold 0.00003 sufficient for test and train\n",
    "    #time_threshold 0.00002 will throw errors\n",
    "    '''\n",
    "    removes time jumps in the simulation for a single satellite\n",
    "    for train and test data, sufficient to set time_threshold at default\n",
    "    s_data = satellite data\n",
    "    the features are replaced by the correction\n",
    "    note that threshold here is not the same as in remove_time_jumps\n",
    "    '''\n",
    "    epoch_ind = data.columns.get_loc('epoch')\n",
    "    data['t'] = ((pd.to_datetime(data['epoch']) - pd.to_datetime(data.iloc[0,epoch_ind])) /\n",
    "                               np.timedelta64(1, 'D')).astype(float)\n",
    "    data['dt'] = data['t'].diff(1)\n",
    "\n",
    "    index_for_correction = data[data['dt'] < threshold].index \n",
    "    #print(index_for_correction)\n",
    "    if list(index_for_correction): #if non empty\n",
    "        for feature in features_list:\n",
    "            for i in index_for_correction:\n",
    "                j = data.index.get_loc(i)\n",
    "                data = insert_previous_and_shift(data,feature,j)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stretch_simulated_feats(data, stretch,true_feats = 'position',\n",
    "                           suffix=\"_stretch\"):\n",
    "    '''\n",
    "    stretch time scale for simulation to better match true data,\n",
    "    for a single sattelite;\n",
    "    use predetermined stretch coefficient (depends on the satellite)\n",
    "    \n",
    "    '''\n",
    "    if true_feats == 'position':\n",
    "        true_feats_list = ['x','y','z']\n",
    "    elif true_feats == 'velocity':\n",
    "        true_feats_list = ['Vx','Vy','Vz']\n",
    "    elif true_feats == 'all':\n",
    "        true_feats_list = ['x','y','z'] + ['Vx','Vy','Vz']\n",
    "    else:\n",
    "        true_feats_list = true_feats\n",
    "    \n",
    "    for feature in true_feats_list:\n",
    "        spl = splrep(stretch*data['t'],data[feature+'_sim'])\n",
    "        test_stretch = splev(data['t'], spl) #np array\n",
    "        data[feature+suffix] = test_stretch\n",
    "    \n",
    "    return data\n",
    "\n",
    "def amp_sim_feats(data, amp_stretch,feats = ['Vx_sim','Vy_sim','Vz_sim'],\n",
    "                           suffix=\"_stretch_amp\"):\n",
    "    '''\n",
    "    vary amplitude for simulation to better match true data,\n",
    "    for a single sattelite;\n",
    "    use predetermined amp_stretch coefficient (depends on the satellite)\n",
    "    \n",
    "    '''\n",
    "    if feats == 'position':\n",
    "        feats_list = ['x_sim','y_sim','z_sim']\n",
    "    elif feats == 'velocity':\n",
    "        feats_list = ['Vx_sim','Vy_sim','Vz_sim']\n",
    "    elif feats == 'all':\n",
    "        feats_list = ['x_sim','y_sim','z_sim'] + ['Vx_sim','Vy_sim','Vz_sim']\n",
    "    else:\n",
    "        feats_list = feats\n",
    "        \n",
    "    for feature in feats_list:\n",
    "        data[feature+suffix] = amp_stretch*data[feature]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_X(df,col='sat_id',ratio = 0.8,discard = 0):\n",
    "    '''\n",
    "    train test split for our train data only, no targets, \n",
    "    default, 80% train for each satellite, 20% test\n",
    "    '''\n",
    "    sat_list = df[col].unique()\n",
    "    X_train = pd.DataFrame([])\n",
    "    X_test = pd.DataFrame([])\n",
    "    for sat in sat_list:\n",
    "        sat_df = get_satellite_data(df,sat)\n",
    "        m = int(discard*sat_df.shape[0])\n",
    "        n = int(ratio*sat_df.shape[0])\n",
    "        X_train = X_train.append(sat_df[m:n])\n",
    "        X_test = X_test.append(sat_df[n:])\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_list = ['x','y','z','Vx','Vy','Vz']\n",
    "feature_list = [t+'_sim' for t in target_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv', index_col = 'id')\n",
    "train['time'] = train['epoch']\n",
    "train['epoch'] = pd.to_datetime(train['epoch']).values.astype(float)\n",
    "test = pd.read_csv('data/test.csv', index_col = 'id')\n",
    "test['time'] = test['epoch']\n",
    "test['epoch'] = pd.to_datetime(test['epoch']).values.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: (649912, 15)\n"
     ]
    }
   ],
   "source": [
    "print('train:',train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Track 1 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_t1 = test['sat_id'].unique()\n",
    "train_t1 = train[train['sat_id'].isin(sat_t1)]\n",
    "train_test = pd.concat([train,test],sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load transformed data, from train (January) and test (February) data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['x_sim', 'y_sim', 'z_sim', 'Vx_sim', 'Vy_sim', 'Vz_sim', 'sat_id',\n",
       "       'epoch'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tf = pd.read_csv('train_transformed.csv')\n",
    "train_tf.drop('id',axis=1,inplace=True)\n",
    "train_tf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tf = pd.read_csv('submission_2020-02-03_12-12-21.csv',index_col='id')\n",
    "test_epoch = pd.read_csv('data/test.csv',index_col='id')['epoch']\n",
    "test_sat_id = pd.read_csv('data/test.csv',index_col='id')['sat_id']\n",
    "test_tf = pd.concat([test_sat_id,test_tf,test_epoch],axis=1)\n",
    "test_tf['epoch'] = pd.to_datetime(test_tf['epoch']).values.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_dict = {c:c_n for (c,c_n) in list(zip(target_list,feature_list))}\n",
    "test_tf = test_tf.rename(rename_dict,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = train.iloc[0,train.columns.get_loc('epoch')]\n",
    "day = (train['epoch'].max() - train['epoch'].min())/31"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tf = train_tf[test_tf.columns]\n",
    "df = pd.concat([train_tf,test_tf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['epoch', 'sat_id','x_sim', 'y_sim',\n",
    "       'z_sim', 'Vx_sim', 'Vy_sim', 'Vz_sim', 'time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tf = train_tf[test_tf.columns]\n",
    "df = pd.concat([train_tf,test_tf])\n",
    "df['r'] = df['x_sim']**2 + df['y_sim']**2 + df['z_sim']**2\n",
    "df['Vr'] = df['Vx_sim']**2 + df['Vy_sim']**2 + df['Vz_sim']**2\n",
    "df['r_ratio_Vr'] = df['r']/df['Vr']\n",
    "df['epoch'] = (df['epoch']-start)/day\n",
    "for col in df.columns:\n",
    "    if col not in ['sat_id','time']:\n",
    "        df[col+'_d1'] = df[col].diff(1)\n",
    "    if col not in ['sat_id','epoch','time']:\n",
    "        df[col+'_d2'] = df[col].diff(2)\n",
    "        df[col+'_d3'] = df[col].diff(3)\n",
    "        df[col+'_d4'] = df[col].diff(4)\n",
    "        df[col+'_d5'] = df[col].diff(5)\n",
    "        df[col+'_shift1'] = df[col].shift(1)\n",
    "        for i in range(2,6):\n",
    "            df[col+'_shift{}'.format(i)] = df[col+'_shift{}'.format(i-1)]\n",
    "df = df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(599254, 102)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fe = df[:train_t1.shape[0]]\n",
    "test_fe = df[train_t1.shape[0]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(284071, 102)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_fe.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train test split: Discard first 60% of data, then 50/50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame([])\n",
    "X_test = pd.DataFrame([])\n",
    "for sat in train_fe['sat_id'].unique():\n",
    "    train,test = train_test_split_X(get_satellite_data(train_fe,sat),ratio = 0.8,discard=0.6)\n",
    "    X_train = X_train.append(train)\n",
    "    X_test = X_test.append(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(63040, 102) (63158, 102)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape,X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['epoch', 'sat_id', 'x', 'y', 'z', 'Vx', 'Vy', 'Vz', 'x_sim', 'y_sim',\n",
       "       'z_sim', 'Vx_sim', 'Vy_sim', 'Vz_sim', 'time'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_t1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pd.DataFrame([])\n",
    "y_test = pd.DataFrame([])\n",
    "for sat in train_fe['sat_id'].unique():\n",
    "    train,test = train_test_split_X(get_satellite_data(train_t1,sat),ratio = 0.8, discard=0.6)\n",
    "    y_train = y_train.append(train)\n",
    "    y_test = y_test.append(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train[target_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(63040, 6) (63158, 15)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9min 25s, sys: 2.78 s, total: 9min 27s\n",
      "Wall time: 2min 53s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=-1,\n",
       "           oob_score=False, random_state=50, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "rf = RandomForestRegressor(n_estimators=50, n_jobs=-1, random_state=50)\n",
    "rf.fit(X_train.fillna(0), y_train['Vx'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smape for RF, Vx: 0.07736737939508413\n",
      "smape for ground truth and simluation, Vx: 0.6774338934855161\n"
     ]
    }
   ],
   "source": [
    "pred = rf.predict(X_test)\n",
    "print('smape for RF, Vx:',smape(pred,y_test['Vx']))\n",
    "print('smape for ground truth and simluation, Vx:',smape(X_test['Vx_sim'],y_test['Vx']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_train = [] #training data simulation \n",
    "for sat in sat_t1:\n",
    "    dfs_train.append(get_satellite_data(train_transformed,sat))\n",
    "dfs_test = [] #test data simulation\n",
    "for sat in sat_t1:\n",
    "    dfs_test.append(get_satellite_data(test_transformed,sat))\n",
    "dfs_target = [] #training data ground truth\n",
    "for sat in sat_t1: \n",
    "    dfs_target.append(get_satellite_data(data[target_list +['sat_id']],sat))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IDAOenv",
   "language": "python",
   "name": "idaoenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
