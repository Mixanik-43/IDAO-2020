{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'data/Track 1/test.csv' does not exist: b'data/Track 1/test.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-370691d583d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/Track 1/test.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    683\u001b[0m         )\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1135\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1136\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1917\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'data/Track 1/test.csv' does not exist: b'data/Track 1/test.csv'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm, trange\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from scipy.interpolate import InterpolatedUnivariateSpline, UnivariateSpline\n",
    "from scipy.optimize import leastsq, least_squares\n",
    "from scipy.signal import savgol_filter\n",
    "import utils\n",
    "from collections import defaultdict\n",
    "\n",
    "np.random.seed(50)\n",
    "coord_cols = ['x', 'y', 'z']\n",
    "speed_cols = ['Vx', 'Vy', 'Vz']\n",
    "state_cols = coord_cols + speed_cols\n",
    "\n",
    "print('Loading data...')\n",
    "train_data = pd.read_csv('data/Track 1/train.csv', index_col='id')\n",
    "train_data['epoch'] = pd.to_datetime(train_data['epoch']).astype(int)\n",
    "\n",
    "test_data = pd.read_csv('data/Track 1/test.csv', index_col='id')\n",
    "test_data['epoch'] = pd.to_datetime(test_data['epoch']).astype(int)\n",
    "\n",
    "data = pd.concat([train_data, test_data], sort=False).reset_index(drop=True)\n",
    "print('Data loaded.')\n",
    "\n",
    "G = 6.6743e-11 # gravity constant\n",
    "M = 5.972e+24  # Earth mass\n",
    "\n",
    "# coord = np.array([x, y, z])\n",
    "# speed = np.array([Vx, Vy, Vz])\n",
    "# sat_state = np.array([x, y, z, Vx, Vy, Vz])\n",
    "def Earth_gravity_model(sat_state, dt):\n",
    "    coord = sat_state[:3]\n",
    "    speed = sat_state[3:]\n",
    "    r = np.linalg.norm(coord)       # distance to satellite\n",
    "    a_abs = (G * M) / (r ** 2)      # acceleration absolutevalue\n",
    "    a = -(coord / r) * a_abs\n",
    "    result_speed = speed + a * dt\n",
    "    result_coord = coord + (speed + result_speed) / 2 * dt\n",
    "    return np.concatenate([result_coord, result_speed])\n",
    "\n",
    "\n",
    "# iterating Earth_gravity_model or other function with same interface\n",
    "def iterative_trajectory_modelling(model, start_state, t_simulation, dt):\n",
    "    current_state = start_state\n",
    "    for step in range(int(t_simulation / dt)):\n",
    "        current_state = model(sat_state=current_state, dt=dt)\n",
    "    current_state = model(current_state, dt=t_simulation % dt)\n",
    "    return current_state\n",
    "\n",
    "# predicting past states\n",
    "def inverse_iterative_trajectory_modelling(model, start_state, t_simulation, dt=1):\n",
    "    start_state[3:] *= -1\n",
    "    end_state = iterative_trajectory_modelling(model, start_state, t_simulation, dt)\n",
    "    end_state[3:] *= -1\n",
    "    return end_state\n",
    "\n",
    "\n",
    "def time_state(row):\n",
    "    time = row['epoch']\n",
    "    state = row[state_cols].values\n",
    "    return time, state\n",
    "    \n",
    "def predict_segment(begin_row, end_row, t_new, dt):\n",
    "    pred = defaultdict(lambda: {})\n",
    "    if begin_row is not None:\n",
    "        current_t, current_state = begin_t, begin_state = time_state(begin_row)\n",
    "        for t in t_new:\n",
    "            if t >= current_t:\n",
    "                current_state = iterative_trajectory_modelling(\n",
    "                    Earth_gravity_model, current_state * 1000, (t - current_t) / 10 ** 9,\n",
    "                    dt=dt) / 1000\n",
    "                current_t = t\n",
    "                pred[current_t]['forward'] = {'pred': current_state, 'sim_duration': current_t - begin_t}\n",
    "    if end_row is not None:\n",
    "        current_t, current_state = end_t, end_state = time_state(end_row)\n",
    "        for t in t_new[::-1]:\n",
    "            if t <= current_t:\n",
    "                current_state = inverse_iterative_trajectory_modelling(\n",
    "                    Earth_gravity_model, current_state * 1000, (current_t - t) / 10 ** 9,\n",
    "                    dt=dt) / 1000\n",
    "                current_t = t\n",
    "                pred[current_t]['backward'] = {'pred': current_state, 'sim_duration': end_t - current_t}\n",
    "                \n",
    "    segment_df = []\n",
    "    for t in sorted(pred.keys()):\n",
    "        assert len(pred[t]) > 0\n",
    "        t_pred = np.zeros(6)\n",
    "        t_weights_sum = 0\n",
    "        \n",
    "        for simulation, simulation_res in pred[t].items():\n",
    "            assert simulation_res['sim_duration'] >= 0\n",
    "            weight = 1 / (simulation_res['sim_duration'] + 1)\n",
    "            t_pred += simulation_res['pred'] * weight\n",
    "            t_weights_sum += weight\n",
    "            \n",
    "        segment_df.append(np.concatenate([[t], t_pred / t_weights_sum]))\n",
    "    return segment_df\n",
    "\n",
    "def sparse_pred_to_dense(sparse_sat_data, t_new, dt):\n",
    "    \n",
    "    \n",
    "    result = []\n",
    "    first_row = sparse_sat_data.iloc[0]\n",
    "    result.extend(predict_segment(None, first_row, t_new[t_new < first_row['epoch']], dt=dt))\n",
    "    \n",
    "#     t_new_id_min = t_new_id_max = 0\n",
    "    for row_id in range(len(sparse_sat_data) - 1):\n",
    "        begin_row = sparse_sat_data.iloc[row_id]\n",
    "        end_row = sparse_sat_data.iloc[row_id + 1]\n",
    "        segment = t_new[(t_new >= begin_row['epoch']) & (t_new < end_row['epoch'])]\n",
    "        result.extend(predict_segment(begin_row, end_row, segment, dt=dt))\n",
    "    result.extend(predict_segment(end_row, None, t_new[t_new >= end_row['epoch']], dt=dt))\n",
    "    return pd.DataFrame(result, columns=['t',] + state_cols)\n",
    "\n",
    "# basic function used in ZeroKeypointsGenerator\n",
    "# finding t: x(t)=0.\n",
    "def get_key_points(t, x):\n",
    "    spl = InterpolatedUnivariateSpline(t, x)\n",
    "    roots = spl.roots()\n",
    "    key_points = roots[1::2]\n",
    "    if len(key_points) < 3:\n",
    "        return key_points, np.zeros_like(key_points)\n",
    "    \n",
    "    outlier_scores = np.abs((key_points[2:] + key_points[:-2] - 2 * key_points[1:-1]) /\n",
    "                             key_points[1:-1])\n",
    "    np.pad(np.abs((key_points[2:] + key_points[:-2] - 2 * key_points[1:-1]) /\n",
    "                             key_points[1:-1]), (2, 2))\n",
    "    \n",
    "    threshold = 3 * np.percentile(outlier_scores, 75) - 2 * np.percentile(outlier_scores, 25)\n",
    "    outliers = (np.convolve(np.pad(outlier_scores > threshold, (2, 2),constant_values=1), [1, 1, 1]) == 3)[2:-2]\n",
    "    return key_points, outliers\n",
    "\n",
    "def linear_params(t, x):\n",
    "    model_func = lambda params, t: (params[0] * t + params[1])\n",
    "    a = (x[-1] - x[0]) / (t[-1] - t[0])\n",
    "    b = x[0] - a * t[0]\n",
    "    init_params = (a, b)\n",
    "    return model_func, init_params\n",
    "\n",
    "# sinusoid + linear\n",
    "def sinusoid_plus_linear_params(t, x):\n",
    "    model_func = lambda params, t: (params[0] *\n",
    "                                    np.sin(params[1] * t + params[2]) +\n",
    "                                    params[3] + t * params[4])\n",
    "    init_params = (np.std(x), 1/(t[-1] - t[0]), 0, np.mean(x), (x[-1] - x[0]) / (t[-1] - t[0]))\n",
    "    return model_func, init_params\n",
    "\n",
    "def pick_model_function(t, x):\n",
    "#     print(len(t))\n",
    "    if len(t) >= 10:\n",
    "        return sinusoid_plus_linear_params(t, x)\n",
    "    else:\n",
    "        return linear_params(t, x)\n",
    "\n",
    "def fit_curve(t, x, model_func, init_params):\n",
    "    def optimize_func(params):\n",
    "        return model_func(params, t) - x\n",
    "\n",
    "    ls_params = leastsq(optimize_func, init_params)[0]\n",
    "    return lambda x: model_func(ls_params, x)\n",
    "\n",
    "class ZeroKeypointsGenerator:\n",
    "    def __init__(self, anchor_feature):\n",
    "        self.anchor_feature = anchor_feature\n",
    "    \n",
    "    def get_sim_keypoints(self, sat_data):\n",
    "        return get_key_points(sat_data['epoch'], sat_data[self.anchor_feature + '_sim'])\n",
    "    \n",
    "    def get_gt_keypoints(self, sat_data):\n",
    "        return get_key_points(sat_data['epoch'], sat_data[self.anchor_feature])\n",
    "    \n",
    "# generating a lattice of keypoints\n",
    "class ShiftZeroKeypointsGenerator:\n",
    "    def __init__(self, anchor_feature, alpha=1):\n",
    "        self.anchor_feature = anchor_feature\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def get_sim_keypoints(self, sat_data):\n",
    "        kp, outliers = get_key_points(sat_data['epoch'], sat_data[self.anchor_feature + '_sim'])\n",
    "        outliers = outliers[1:] | outliers[:-1]\n",
    "        period = kp[1:] - kp[:-1]\n",
    "        return kp[:-1] + period * self.alpha, outliers\n",
    "    \n",
    "    def get_gt_keypoints(self, sat_data):\n",
    "        kp, outliers = get_key_points(sat_data['epoch'], sat_data[self.anchor_feature])\n",
    "        outliers = outliers[1:] | outliers[:-1]\n",
    "        period = kp[1:] - kp[:-1]\n",
    "        return kp[:-1] + period * self.alpha, outliers\n",
    "    \n",
    "    \n",
    "def sine_alignment(sat_id, kp_generator, train_t_max):\n",
    "    sat_data = utils.get_satellite_data(data, sat_id).reset_index(drop=True)\n",
    "    sat_data = utils.remove_time_jumps_fast(sat_data)\n",
    "    \n",
    "    train_sat_data = sat_data[sat_data['epoch'] <= train_t_max]\n",
    "    \n",
    "    all_sim_kp, all_sim_kp_outliers = kp_generator.get_sim_keypoints(sat_data)\n",
    "    # broken simulation handling\n",
    "    if sat_id == 481:\n",
    "        pred = sat_data[sat_data['epoch'] > train_t_max][['epoch'] + [c + '_sim' for c in state_cols]]\n",
    "        pred.columns =  ['t'] + state_cols\n",
    "        return pred\n",
    "        \n",
    "    train_gt_kp, train_gt_kp_outliers = kp_generator.get_gt_keypoints(train_sat_data)\n",
    "    \n",
    "    stretch_data = all_sim_kp[:len(train_gt_kp)], train_gt_kp\n",
    "    if len(train_gt_kp) >= 5:\n",
    "        use_kp = ~(all_sim_kp_outliers[:len(train_gt_kp)] | train_gt_kp_outliers)\n",
    "        stretch_data = (stretch_data[0][use_kp], stretch_data[1][use_kp])\n",
    "    time_stretch_function = fit_curve(*stretch_data,\n",
    "                                      *pick_model_function(all_sim_kp[:len(train_gt_kp)], train_gt_kp))\n",
    "\n",
    "    \n",
    "    keypoints = time_stretch_function(all_sim_kp)\n",
    "    train_keypoints = keypoints[keypoints < train_t_max]\n",
    "    test_keypoints = keypoints[len(train_keypoints):]\n",
    "    \n",
    "    \n",
    "    \n",
    "    sim_stretched_t = time_stretch_function(sat_data['epoch'])\n",
    "    train_sim_stretched_t = sim_stretched_t[:len(train_sat_data)]\n",
    "    \n",
    "    pred = []\n",
    "    gt = []\n",
    "    for feature in state_cols:\n",
    "        sim_feature = feature + '_sim'\n",
    "\n",
    "        \n",
    "        # values of simulation at all key points\n",
    "        all_kp_sim_feature = utils.resample(t=sim_stretched_t.values,\n",
    "                                              x=sat_data[sim_feature].values,\n",
    "                                              t_new=keypoints)\n",
    "        # values of simulation at train key points\n",
    "        train_kp_sim_feature = all_kp_sim_feature[:len(train_keypoints)]\n",
    "        \n",
    "        # ground truth values at train key points\n",
    "        train_kp_gt_feature = utils.resample(t=train_sat_data['epoch'],\n",
    "                                             x=train_sat_data[feature],\n",
    "                                             t_new=train_keypoints)\n",
    "\n",
    "        # difference between train and ground truth at train keypoints\n",
    "        train_diff = train_kp_gt_feature - train_kp_sim_feature\n",
    "#         kp_diff_func = lambda x: np.ones_like(x) * np.mean(train_diff)\n",
    "        kp_diff_func = fit_curve(train_keypoints, train_diff,\n",
    "                                  *linear_params(train_keypoints, train_diff))\n",
    "        pred_kp_diff = kp_diff_func(test_keypoints)\n",
    "\n",
    "\n",
    "        pred.append(pred_kp_diff + all_kp_sim_feature[len(train_keypoints):])\n",
    "    pred_df = pd.DataFrame(np.array(pred), index=state_cols).T\n",
    "    pred_df['epoch'] = test_keypoints\n",
    "    return pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Begin modeling. Expected to take several hours.')\n",
    "for sat_id in tqdm(test_data['sat_id'].unique()):\n",
    "    try:\n",
    "        train_t = utils.get_satellite_data(train_data, sat_id)['epoch']\n",
    "        test_t = utils.get_satellite_data(test_data, sat_id)['epoch']\n",
    "        pred_dfs = []\n",
    "        sparse_pred_dfs = []\n",
    "        # running sine_alignment for different lattices:\n",
    "        # different alphas and anchor features\n",
    "        for anchor in state_cols:\n",
    "            for alpha in np.linspace(0, 1, 100)[1:]:\n",
    "                pred_df = sine_alignment(sat_id, ShiftZeroKeypointsGenerator(anchor, alpha), train_t.max())\n",
    "                sparse_pred_dfs.append(pred_df)\n",
    "        sparse_pred = pd.concat(sparse_pred_dfs).sort_values('epoch').reset_index(drop=True)\n",
    "        dense_pred = sparse_pred_to_dense(sparse_pred, test_t, dt=8)\n",
    "        all_predictions_shiftzero_kp[sat_id] = dense_pred\n",
    "        #backup save\n",
    "        #with open(f'data/Track 1/tmp/shiftzero_keypoints/{sat_id}.pkl', 'wb') as f:\n",
    "        #    pkl.dump(dense_pred, f)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "print('Satellite 481 with broken simulation...')\n",
    "# satellite 481 has broken simulation.\n",
    "all_predictions_shiftzero_kp[481] = sine_alignment(481, ShiftZeroKeypointsGenerator('x', 0), train_t.max())\n",
    "print('Modeling complete...')\n",
    "\n",
    "#\"problem satellites\", based on smape, identified by rerunning sine alingment on first 75% of January, predicting next 25% of January\n",
    "problem_sat = sorted([1,587, 372,  37, 473, 523, 514, 253, 481,  35, 515, 162, 277, 244, 443, 572, 362, 550,  26, 310, 252, 517, 127, 396, 391, 471, 333, 28,  54, 6, 502, 316, 225,  82, 309, 268, 470, 456, 460, 510,516,  22, 194, 511, 544, 438, 435, 486,  52, 548, 528])\n",
    "\n",
    "print('Smoothing for 51 \"problem satellites\"...')\n",
    "#smoothing here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('Satellite 310 smoothing')\n",
    "def sine_alignment_part(sat_id, kp_generator, train_t_max,train_t_min = 0):\n",
    "    '''\n",
    "    Args:\n",
    "    sat_id\n",
    "    kp_generator = instance of ShiftZeroKeypointsGenerator class\n",
    "    train_t_max = max time to train on\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    sat_data = utils.get_satellite_data(data, sat_id).reset_index(drop=True)\n",
    "    sat_data = utils.remove_time_jumps_fast(sat_data)\n",
    "    \n",
    "    train_sat_data = sat_data[sat_data['epoch'] <= train_t_max]\n",
    "    m = int(train_t_min*train_sat_data.shape[0])\n",
    "    train_sat_data = train_sat_data[m:]\n",
    "    \n",
    "    all_sim_kp, all_sim_kp_outliers = kp_generator.get_sim_keypoints(sat_data)\n",
    "    # broken simulation handling\n",
    "    if sat_id == 481:\n",
    "        pred = sat_data[sat_data['epoch'] > train_t_max][['epoch'] + [c + '_sim' for c in state_cols]]\n",
    "        pred.columns =  ['t'] + state_cols\n",
    "        return pred\n",
    "        \n",
    "    train_gt_kp, train_gt_kp_outliers = kp_generator.get_gt_keypoints(train_sat_data)\n",
    "    \n",
    "    stretch_data = all_sim_kp[:len(train_gt_kp)], train_gt_kp #sim, followed by train  keypts\n",
    "    if len(train_gt_kp) >= 5:\n",
    "        use_kp = ~(all_sim_kp_outliers[:len(train_gt_kp)] | train_gt_kp_outliers)\n",
    "        stretch_data = (stretch_data[0][use_kp], stretch_data[1][use_kp])\n",
    "    time_stretch_function = fit_curve(*stretch_data,\n",
    "                                      *pick_model_function(all_sim_kp[:len(train_gt_kp)], train_gt_kp,ln=False))\n",
    "    \n",
    "    keypoints = time_stretch_function(all_sim_kp)\n",
    "    train_keypoints = keypoints[keypoints < train_t_max]\n",
    "    test_keypoints = keypoints[len(train_keypoints):]\n",
    "    \n",
    "    sim_stretched_t = time_stretch_function(sat_data['epoch'])\n",
    "    train_sim_stretched_t = sim_stretched_t[:len(train_sat_data)]\n",
    "    \n",
    "    pred = []\n",
    "    gt = []\n",
    "    for feature in state_cols:\n",
    "        sim_feature = feature + '_sim'\n",
    "\n",
    "        \n",
    "        # values of simulation at all key points\n",
    "        all_kp_sim_feature = utils.resample(t=sim_stretched_t.values,\n",
    "                                              x=sat_data[sim_feature].values,\n",
    "                                              t_new=keypoints)\n",
    "        # values of simulation at train key points\n",
    "        train_kp_sim_feature = all_kp_sim_feature[:len(train_keypoints)]\n",
    "        \n",
    "        # ground truth values at train key points\n",
    "        train_kp_gt_feature = utils.resample(t=train_sat_data['epoch'],\n",
    "                                             x=train_sat_data[feature],\n",
    "                                             t_new=train_keypoints)\n",
    "\n",
    "        # difference between train and ground truth at train keypoints\n",
    "        train_diff = train_kp_gt_feature - train_kp_sim_feature\n",
    "#         kp_diff_func = lambda x: np.ones_like(x) * np.mean(train_diff)\n",
    "        kp_diff_func = fit_curve(train_keypoints, train_diff,\n",
    "                                  *linear_params(train_keypoints, train_diff))\n",
    "        pred_kp_diff = kp_diff_func(test_keypoints)\n",
    "\n",
    "\n",
    "        pred.append(pred_kp_diff + all_kp_sim_feature[len(train_keypoints):])\n",
    "    pred_df = pd.DataFrame(np.array(pred), index=state_cols).T\n",
    "    pred_df['epoch'] = test_keypoints\n",
    "    return pred_df\n",
    "\n",
    "sat_id = 310\n",
    "try:\n",
    "    train_t = utils.get_satellite_data(train_data, sat_id)['epoch']\n",
    "    test_t = utils.get_satellite_data(test_data, sat_id)['epoch']\n",
    "    pred_dfs = []\n",
    "    sparse_pred_dfs = []\n",
    "    # running sine_alignment for different lattices:\n",
    "    # different alphas and anchor features\n",
    "    for anchor in state_cols:\n",
    "        #print(anchor)\n",
    "        for alpha in np.linspace(0, 1, 100)[1:]:\n",
    "            pred_df = sine_alignment_part(sat_id, ShiftZeroKeypointsGenerator(anchor, alpha), train_t.max(),0.6)\n",
    "            #print(pred_df.shape)\n",
    "            sparse_pred_dfs.append(pred_df)\n",
    "    sparse_pred = pd.concat(sparse_pred_dfs).sort_values('epoch').reset_index(drop=True)\n",
    "    dense_pred = sparse_pred_to_dense(sparse_pred, test_t, dt=8)\n",
    "    dense_pred['x'] = savgol_filter(dense_pred['x'],25,3)\n",
    "    dense_pred['Vx'] = savgol_filter(dense_pred['Vx'],25,3)\n",
    "    all_predictions_shiftzero_kp[sat_id] = dense_pred\n",
    "\n",
    "    #backup save\n",
    "    #with open(f'data/Track 1/tmp/shiftzero_keypoints/{sat_id}.pkl', 'wb') as f:\n",
    "    #    pkl.dump(dense_pred, f)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to script submission.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IDAOenv",
   "language": "python",
   "name": "idaoenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
